{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6e45f3-7b68-43ee-acd2-5eabb6cf0090",
   "metadata": {},
   "source": [
    "Q1. R-squared in Linear Regression:\n",
    "\n",
    "R-squared (coefficient of determination) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.\n",
    "It is calculated as the ratio of the explained variance to the total variance.\n",
    "The formula for R-squared is: \n",
    "R2=1− SSres/SStotal, where SSres is the sum of squared residuals (the difference between the observed and predicted values) and \n",
    "\n",
    "SStot is the total sum of squares (the difference between the observed values and the mean of the dependent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585eebf8-e7fa-44ff-a641-454e762c7354",
   "metadata": {},
   "source": [
    "Q2. Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of predictors in the model.\n",
    "It penalizes the addition of irrelevant predictors that do not improve the model significantly.\n",
    "The formula for adjusted R-squared is: \n",
    "Adjusted R \n",
    "2\n",
    " =1−(1−R2)(n−1)/n-k-1, where \n",
    "\n",
    "n is the number of observations and \n",
    "\n",
    "k is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc10d1e-7548-4242-8d55-98bfcd244399",
   "metadata": {},
   "source": [
    "Q3. When to use Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors.\n",
    "It helps in preventing the overestimation of th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26316fb0-d335-4cf6-beac-ee8092463c70",
   "metadata": {},
   "source": [
    "Q4. RMSE, MSE, and MAE:\n",
    "\n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to evaluate the performance of regression models.\n",
    "RMSE is the square root of the average of squared differences between predicted and actual values. MSE is the average of squared differences, and MAE is the average of absolute differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab37a753-4d58-4fc0-8763-fc004b505b6f",
   "metadata": {},
   "source": [
    "Q5. Advantages and Disadvantages of Evaluation Metrics:\n",
    "\n",
    "RMSE gives more weight to large errors, making it sensitive to outliers. MSE also emphasizes larger errors. MAE is less sensitive to outliers but may not penalize large errors enough.\n",
    "RMSE and MSE are useful when large errors are critical, while MAE is often preferred when all errors, regardless of size, should be treated equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a39fb8-9a6c-41b3-a6c1-83406d8994c9",
   "metadata": {},
   "source": [
    "Q6. Lasso Regularization:\n",
    "\n",
    "Lasso regularization adds a penalty term to the linear regression cost function that is proportional to the absolute values of the coefficients.\n",
    "It can force some coefficients to be exactly zero, effectively performing feature selection.\n",
    "Lasso differs from Ridge regularization, which adds a penalty term proportional to the square of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd9159-b3ec-4a2c-b646-98cc5555f4f8",
   "metadata": {},
   "source": [
    "Q7. Regularized Linear Models for Overfitting:\n",
    "\n",
    "Regularized linear models help prevent overfitting by penalizing large coefficients, which can lead to a more generalized model.\n",
    "For example, Lasso and Ridge regression can be used to shrink coefficients towards zero, reducing the model's complexity and preventing it from fitting the noise in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9362e-7afc-4426-a06e-8f8282005540",
   "metadata": {},
   "source": [
    "Q8. Limitations of Regularized Linear Models:\n",
    "\n",
    "Regularization may not work well when there are strong multicollinearity issues.\n",
    "The choice of the regularization parameter is crucial, and it may not always be easy to determine the optimal value.\n",
    "In some cases, a simpler model without regularization might perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4cceb-d6d4-4006-864b-b2a1b35d55b5",
   "metadata": {},
   "source": [
    "Q9. Comparing Models with Different Metrics:\n",
    "\n",
    "The choice depends on the specific context. RMSE emphasizes larger errors, while MAE treats all errors equally. The decision should consider the importance of different error sizes in the given application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443ece9-0b71-4a26-bec5-2158edd0e862",
   "metadata": {},
   "source": [
    "Q10. Comparing Regularized Linear Models:\n",
    "- The choice depends on the specific problem and the importance of sparsity. Lasso tends to produce sparse models by driving some coefficients to exactly zero. Ridge may perform better when there are many small coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb30c1d-01a7-40db-9dcf-b8a2e11c1b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
